# -*- coding: utf-8 -*-
"""RS A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAxdTcgHmza-P6WPeekJb9FFOa98twT4
"""

!pip install pandas matplotlib seaborn

!pip install scikit-surprise

"""# Q1) MovieLense Data set"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load ratings dataset from GitHub
url = "https://raw.githubusercontent.com/sidooms/MovieTweetings/master/snapshots/100K/ratings.dat"
df = pd.read_csv(url, sep='::', engine='python', names=['user_id', 'movie_id', 'rating', 'timestamp'])

# Preview the data
df.head()

# General info
print("Shape:", df.shape)
print(df.info())
print(df.describe())
print("\nMissing values:\n", df.isnull().sum())

# Ratings distribution
sns.histplot(df['rating'], bins=10, kde=True)
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

# Ratings per user
user_counts = df['user_id'].value_counts()
sns.histplot(user_counts, bins=50)
plt.title('Number of Ratings per User')
plt.xlabel('Ratings')
plt.ylabel('Users')
plt.show()

# Ratings per movie
movie_counts = df['movie_id'].value_counts()
sns.histplot(movie_counts, bins=50)
plt.title('Number of Ratings per Movie')
plt.xlabel('Ratings')
plt.ylabel('Movies')
plt.show()

"""# Q1) FILMTRUST DATA SET"""

from google.colab import files
uploaded = files.upload()

import zipfile
import os

# Unzip the uploaded file
with zipfile.ZipFile("archive (2).zip", 'r') as zip_ref:
    zip_ref.extractall("filmtrust")

os.listdir("filmtrust")

import pandas as pd


df = pd.read_csv("filmtrust/ratings.txt", sep=' ', names=['user_id', 'item_id', 'rating'])
df.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Basic info
print("Shape:", df.shape)
print(df.info())
print(df.describe())
print("\nMissing values:\n", df.isnull().sum())

# Ratings distribution
sns.histplot(df['rating'], bins=10, kde=True)
plt.title('Rating Distribution - FilmTrust')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Ratings per user
user_counts = df['user_id'].value_counts()
sns.histplot(user_counts, bins=50)
plt.title('Ratings per User - FilmTrust')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Users')
plt.show()

# Ratings per item
item_counts = df['item_id'].value_counts()
sns.histplot(item_counts, bins=50)
plt.title('Ratings per Item - FilmTrust')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Items')
plt.show()

"""# Q1) BookCrossing DATASET"""

from google.colab import files
uploaded = files.upload()

import zipfile
import os

# Unzipping the file
with zipfile.ZipFile("archive (3).zip", 'r') as zip_ref:
    zip_ref.extractall("/content/book-crossing")

# Check the extracted files
os.listdir("/content/book-crossing")

import pandas as pd

# Load the datasets
books = pd.read_csv('/content/book-crossing/Books.csv', sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)
users = pd.read_csv('/content/book-crossing/Users.csv', sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)
ratings = pd.read_csv('/content/book-crossing/Ratings.csv', sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)

# Preview the data
print("Books Dataset:")
print(books.head())

print("\nUsers Dataset:")
print(users.head())

print("\nRatings Dataset:")
print(ratings.head())

# Check the shape of each dataset
print(f"Books: {books.shape}")
print(f"Users: {users.shape}")
print(f"Ratings: {ratings.shape}")

# Check for missing values
print("\nMissing values in Books:")
print(books.isnull().sum())

print("\nMissing values in Users:")
print(users.isnull().sum())

print("\nMissing values in Ratings:")
print(ratings.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

# Convert 'Book-Rating' to numeric if it's not already
ratings['Rating'] = pd.to_numeric(ratings['Rating'], errors='coerce')

# Plot the distribution of book ratings
plt.figure(figsize=(10,6))
sns.countplot(data=ratings, x='Rating')
plt.title('Distribution of Book Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

# Merge ratings with books to get book titles
ratings_books = pd.merge(ratings, books, on='ISBN')

# Calculate the number of ratings per book
book_rating_counts = ratings_books['Title'].value_counts().head(10)

# Plot the top 10 most rated books
plt.figure(figsize=(10,6))
sns.barplot(x=book_rating_counts.values, y=book_rating_counts.index)
plt.title('Top 10 Most Rated Books')
plt.xlabel('Number of Ratings')
plt.ylabel('Book Title')
plt.show()

"""# Q2 ) NBCF"""

import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

# Load the MovieLens data
url = "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
!wget -q $url -O movielens.zip
!unzip -q movielens.zip

# Load the movie ratings data (the 'ratings.csv' file)
ratings_df = pd.read_csv("ml-latest-small/ratings.csv")
ratings_df.head()

# Create a user-item matrix (pivot the data)
user_item_matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')

# Fill missing values with 0 (indicating unrated items)
user_item_matrix = user_item_matrix.fillna(0)
user_item_matrix.head()

# Flatten the matrix for model training
user_item_matrix_flattened = user_item_matrix.values.flatten()
user_ids, item_ids = np.where(user_item_matrix != 0)
ratings = user_item_matrix.values[user_ids, item_ids]

# Split the data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(np.column_stack((user_ids, item_ids)), ratings, test_size=0.2, random_state=42)

# Check the shape of training and testing data
print(X_train.shape, X_test.shape)

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test) # Transform y_test using the same encoder

# Train Na√Øve Bayes classifier
model = GaussianNB()

# Fit the model
model.fit(X_train, y_train)

# Predict ratings on the test data
y_pred = model.predict(X_test)

# Convert predictions back to original rating scale for evaluation
y_pred = le.inverse_transform(y_pred)  # Inverse transform to get original ratings
y_test = le.inverse_transform(y_test)

# Evaluate the model performance using Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")